# Part 2: Coding Attention Mechanisms

&nbsp;
## Main Code

- [01_main-code](01_main-code) contains the main part code.

&nbsp;
## Bonus Materials

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) implements and compares different implementation variants of multihead-attention
- [03_understanding-buffers](03_understanding-buffers) explains the idea behind PyTorch buffers, which are used to implement the causal attention mechanism in Part 2